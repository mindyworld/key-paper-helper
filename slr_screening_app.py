"""
ğŸ”‘ Key-Paper Helper (í•„ë…ë…¼ë¬¸ ì°¾ê¸°)
í‚¤ì›Œë“œ ì…ë ¥ â†’ í•„ë… ë…¼ë¬¸ Top 10 ì¶”ì²œ
"""

import streamlit as st
import requests
import pandas as pd
import time
from datetime import datetime
from collections import Counter
from itertools import combinations
import networkx as nx

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Key-Paper Helper",
    page_icon="ğŸ”‘",
    layout="wide"
)

# OpenAlex ê²€ìƒ‰ í•¨ìˆ˜
@st.cache_data(ttl=3600)
def search_openalex(query, year_from, year_to, search_title=True, search_abstract=True, search_keyword=False, max_results=500):
    """OpenAlex API ê²€ìƒ‰"""
    BASE_URL = "https://api.openalex.org/works"
    all_results = []
    cursor = "*"
    total_count = 0
    
    # ê¸°ë³¸ í•„í„°
    base_filter = f"publication_year:{year_from}-{year_to}"
    
    while len(all_results) < max_results:
        params = {
            "search": query,  # ê¸°ë³¸ ê²€ìƒ‰ (ì œëª©+ì´ˆë¡+ì „ë¬¸)
            "filter": base_filter,
            "per_page": 200,
            "cursor": cursor,
            "select": "id,doi,title,publication_year,cited_by_count,type,authorships,primary_location,abstract_inverted_index"
        }
        
        # ì œëª©ë§Œ ê²€ìƒ‰í•˜ëŠ” ê²½ìš°
        if search_title and not search_abstract and not search_keyword:
            params.pop("search", None)
            params["filter"] = f"{base_filter},title.search:{query}"
        # ì´ˆë¡ë§Œ ê²€ìƒ‰í•˜ëŠ” ê²½ìš°
        elif search_abstract and not search_title and not search_keyword:
            params.pop("search", None)
            params["filter"] = f"{base_filter},abstract.search:{query}"
        # ê·¸ ì™¸ëŠ” ê¸°ë³¸ search ì‚¬ìš© (ì œëª©+ì´ˆë¡ ëª¨ë‘ ê²€ìƒ‰)
        
        try:
            response = requests.get(BASE_URL, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            total_count = data.get("meta", {}).get("count", 0)
        except Exception as e:
            st.error(f"API ì˜¤ë¥˜: {e}")
            return [], 0
        
        results = data.get("results", [])
        if not results:
            break
            
        all_results.extend(results)
        cursor = data.get("meta", {}).get("next_cursor")
        if not cursor:
            break
        
        time.sleep(0.1)
    
    return all_results[:max_results], total_count

def reconstruct_abstract(inverted_index):
    """ì´ˆë¡ ë³µì›"""
    if not inverted_index:
        return ""
    word_positions = []
    for word, positions in inverted_index.items():
        for pos in positions:
            word_positions.append((pos, word))
    word_positions.sort(key=lambda x: x[0])
    return " ".join([word for _, word in word_positions])

def process_results(results):
    """ê²°ê³¼ ì²˜ë¦¬"""
    processed = []
    for work in results:
        authors = []
        author_names = []
        for authorship in work.get("authorships", [])[:10]:
            author = authorship.get("author", {})
            if author.get("display_name"):
                authors.append(author["display_name"])
                author_names.append(author["display_name"])
        
        location = work.get("primary_location", {}) or {}
        source = location.get("source", {}) or {}
        
        # ë…¼ë¬¸ ìœ í˜• ë¶„ë¥˜
        title_lower = (work.get("title", "") or "").lower()
        work_type = work.get("type", "")
        
        if "review" in title_lower or "overview" in title_lower or "state-of-the-art" in title_lower:
            paper_type = "Review"
        elif "framework" in title_lower or "model" in title_lower or "theory" in title_lower:
            paper_type = "Framework"
        elif "assess" in title_lower or "evaluat" in title_lower or "measur" in title_lower or "effectiveness" in title_lower:
            paper_type = "Eval"
        else:
            paper_type = "Research"
        
        processed.append({
            "id": work.get("id", ""),
            "title": work.get("title", ""),
            "year": work.get("publication_year"),
            "cited_by_count": work.get("cited_by_count", 0),
            "type": paper_type,
            "authors": "; ".join(authors[:5]),
            "author_list": author_names,
            "journal": source.get("display_name", ""),
            "doi": work.get("doi", ""),
            "abstract": reconstruct_abstract(work.get("abstract_inverted_index"))
        })
    
    return pd.DataFrame(processed)

def calculate_author_centrality(df):
    """ì €ì ì¤‘ì‹¬ì„± ê³„ì‚°"""
    G = nx.Graph()
    
    # ê³µì € ê´€ê³„ë¡œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•
    for _, row in df.iterrows():
        authors = row['author_list'][:5]
        for author in authors:
            if not G.has_node(author):
                G.add_node(author)
        if len(authors) >= 2:
            for pair in combinations(authors, 2):
                if G.has_edge(pair[0], pair[1]):
                    G[pair[0]][pair[1]]['weight'] += 1
                else:
                    G.add_edge(pair[0], pair[1], weight=1)
    
    # ê³ ë¦½ ë…¸ë“œ ì œê±°
    G.remove_nodes_from(list(nx.isolates(G)))
    
    if len(G.nodes()) < 2:
        return [], []
    
    # ì—°ê²° ì¤‘ì‹¬ì„± (Degree Centrality)
    degree_cent = nx.degree_centrality(G)
    top_degree = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:5]
    
    # ë§¤ê°œ ì¤‘ì‹¬ì„± (Betweenness Centrality)
    betweenness_cent = nx.betweenness_centrality(G)
    top_betweenness = sorted(betweenness_cent.items(), key=lambda x: x[1], reverse=True)[:5]
    
    return top_degree, top_betweenness

# í˜„ì¬ ì—°ë„
current_year = datetime.now().year

# ==================== ì‚¬ì´ë“œë°” ====================
with st.sidebar:
    st.markdown("### ğŸ”‘ Key-Paper Helper")
    
    # ğŸ” ê²€ìƒ‰ì–´
    st.markdown("**ğŸ” ê²€ìƒ‰ì–´**")
    search_query = st.text_input(
        "ê²€ìƒ‰ì–´ ì…ë ¥",
        value="",
        placeholder="í‚¤ì›Œë“œ ì…ë ¥",
        label_visibility="collapsed",
        help='ì •í™•í•œ êµ¬ë¬¸ì€ ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„¸ìš”'
    )
    
    # ê²€ìƒ‰ ë²”ìœ„
    st.caption("ê²€ìƒ‰ ë²”ìœ„:")
    search_title = st.checkbox("ì œëª©", value=True, help="ë…¼ë¬¸ ì œëª©ì—ì„œ ê²€ìƒ‰ì–´ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ê°€ì¥ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì–´ìš”.")
    search_abstract = st.checkbox("ì´ˆë¡", value=True, help="ë…¼ë¬¸ ì´ˆë¡(Abstract)ì—ì„œ ê²€ìƒ‰ì–´ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ë” ë„“ì€ ë²”ìœ„ì˜ ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ìˆì–´ìš”.")
    search_keyword = st.checkbox("ì£¼ì œ íƒœê·¸", value=False, help="OpenAlexê°€ ìë™ìœ¼ë¡œ ë¶„ë¥˜í•œ ì£¼ì œ íƒœê·¸(Concepts)ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì €ì í‚¤ì›Œë“œê°€ ì•„ë‹Œ, AIê°€ ë¶€ì—¬í•œ í•™ë¬¸ ë¶„ì•¼ íƒœê·¸ì…ë‹ˆë‹¤.")
    
    st.markdown("")
    
    # ğŸ“… ì¶œíŒ ê¸°ê°„
    st.markdown("**ğŸ“… ì¶œíŒ ê¸°ê°„**")
    
    col1, col2 = st.columns([1, 1])
    with col1:
        year_from = st.selectbox(
            "ì‹œì‘",
            options=list(range(current_year, 1999, -1)),
            index=10,
            label_visibility="collapsed"
        )
    with col2:
        year_to = st.selectbox(
            "ì¢…ë£Œ", 
            options=list(range(current_year, 1999, -1)),
            index=0,
            label_visibility="collapsed"
        )
    
    # ë¹ ë¥¸ ì„ íƒ
    st.caption("ë¹ ë¥¸ ì„ íƒ:")
    last_5y = st.checkbox("ìµœê·¼ 5ë…„", value=False)
    last_10y = st.checkbox("ìµœê·¼ 10ë…„", value=False)
    last_15y = st.checkbox("ìµœê·¼ 15ë…„", value=False)
    
    if last_5y:
        year_from, year_to = current_year - 5, current_year
    elif last_10y:
        year_from, year_to = current_year - 10, current_year
    elif last_15y:
        year_from, year_to = current_year - 15, current_year
    
    st.markdown("---")
    
    search_button = st.button("ğŸ” ê²€ìƒ‰í•˜ê¸°", type="primary", use_container_width=True)
    
    st.markdown("")
    
    if st.button("ğŸ  ì²˜ìŒìœ¼ë¡œ", use_container_width=True):
        st.session_state.clear()
        st.rerun()
    
    st.markdown("")
    st.info("ğŸ’¡ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê³  ë²„íŠ¼ì„ ëˆ„ë¥´ë©´\n**ì¸ìš© ìˆ˜ê°€ ë†’ì€ ìˆœ**ìœ¼ë¡œ\ní•„ë… ë…¼ë¬¸ì„ ë³´ì—¬ë“œë ¤ìš”!")

# ==================== ë©”ì¸ ì˜ì—­ ====================

if search_button and search_query.strip():
    with st.spinner(f"ğŸ“š '{search_query}' ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ëŠ” ì¤‘..."):
        results, total_count = search_openalex(
            search_query, year_from, year_to,
            search_title=search_title,
            search_abstract=search_abstract,
            search_keyword=search_keyword,
            max_results=500
        )
    
    if results:
        df = process_results(results)
        df_sorted = df.sort_values('cited_by_count', ascending=False)
        
        # í—¤ë”
        st.markdown(f"""
        <h1 style="font-size: 3.5rem; font-weight: bold; color: #1e3a5f; text-align: center; margin-bottom: 0;">
            ğŸ”‘ Key-Paper Helper
        </h1>
        <h2 style="font-size: 1.8rem; color: #555; text-align: center; margin-top: 0.5rem; margin-bottom: 1.5rem; font-weight: normal;">
            '{search_query}' ê²€ìƒ‰ ê²°ê³¼ ({year_from}-{year_to}) | ì´ {total_count:,}ê±´
        </h2>
        """, unsafe_allow_html=True)
        
        # 3ê°œ íƒ­
        tab1, tab2, tab3 = st.tabs(["â­ í•„ë… ë…¼ë¬¸ Top 10", "ğŸ‘¥ í•µì‹¬ ì—°êµ¬ì", "ğŸ“š ì£¼ìš” ì €ë„"])
        
        # ========== íƒ­ 1: í•„ë… ë…¼ë¬¸ Top 10 ==========
        with tab1:
            # í—¤ë” + ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (ì˜¤ë¥¸ìª½ ìƒë‹¨)
            col_title, col_download = st.columns([3, 1])
            with col_title:
                st.markdown("### â­ í•„ë… ë…¼ë¬¸ Top 10")
            with col_download:
                csv_data = df_sorted[['title', 'year', 'cited_by_count', 'authors', 'journal', 'doi', 'abstract']].to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    "ğŸ“¥ ì „ì²´ ë‹¤ìš´ë¡œë“œ",
                    data=csv_data,
                    file_name=f"KeyPaper_{search_query[:15].replace(' ', '_')}.csv",
                    mime="text/csv",
                    help="ê²€ìƒ‰ëœ ì „ì²´ ë…¼ë¬¸ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ì œëª©, ì—°ë„, ì¸ìš©ìˆ˜, ì €ì, ì €ë„, DOI, ì´ˆë¡ í¬í•¨)"
                )
            
            top10 = df_sorted.head(10).copy()
            top10['ìˆœìœ„'] = range(1, len(top10) + 1)
            top10['ì¸ìš©'] = top10['cited_by_count'].apply(lambda x: f"{x:,}íšŒ")
            top10['ë…¼ë¬¸'] = top10.apply(lambda r: f"{r['title']} ({r['year']})", axis=1)
            
            # í…Œì´ë¸” í‘œì‹œ
            st.dataframe(
                top10[['ìˆœìœ„', 'ì¸ìš©', 'ë…¼ë¬¸']],
                use_container_width=True,
                hide_index=True,
                column_config={
                    "ìˆœìœ„": st.column_config.NumberColumn("ìˆœìœ„", width="small"),
                    "ì¸ìš©": st.column_config.TextColumn("ì¸ìš©", width="small"),
                    "ë…¼ë¬¸": st.column_config.TextColumn("ë…¼ë¬¸", width="large"),
                }
            )
            
            st.markdown("---")
            
            # ìƒì„¸ ì •ë³´
            st.markdown("#### ğŸ“„ ìƒì„¸ ì •ë³´")
            for i, (_, row) in enumerate(df_sorted.head(10).iterrows(), 1):
                with st.expander(f"**{i}. {row['title']}** ({row['cited_by_count']:,}íšŒ ì¸ìš©)"):
                    # DOI ë§í¬
                    if row['doi']:
                        doi_url = row['doi'] if row['doi'].startswith('http') else f"https://doi.org/{row['doi']}"
                        st.markdown(f"ğŸ”— **[ë…¼ë¬¸ ë°”ë¡œê°€ê¸°]({doi_url})**")
                    
                    st.markdown(f"""
                    - ğŸ“… **ì¶œíŒì—°ë„:** {row['year']}ë…„
                    - ğŸ“– **ì €ë„:** {row['journal'] if row['journal'] else 'N/A'}
                    - ğŸ‘¥ **ì €ì:** {row['authors'][:150]}{'...' if len(row['authors']) > 150 else ''}
                    """)
                    if row['abstract']:
                        st.markdown("**ğŸ“ ì´ˆë¡:**")
                        st.write(row['abstract'][:600] + "..." if len(row['abstract']) > 600 else row['abstract'])
        
        # ========== íƒ­ 2: í•µì‹¬ ì—°êµ¬ì ==========
        with tab2:
            st.markdown("### ğŸ‘¥ í•µì‹¬ ì—°êµ¬ì (Key Players)")
            
            with st.spinner("ì—°êµ¬ì ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘..."):
                top_degree, top_betweenness = calculate_author_centrality(df_sorted)
            
            if top_degree:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### ğŸ¯ ì—°ê²° ì¤‘ì‹¬ì„± Top 5")
                    st.caption("ê°€ì¥ ë§ì€ í˜‘ì—…ì„ í•œ ì—°êµ¬ì")
                    
                    degree_df = pd.DataFrame([
                        {"ìˆœìœ„": i+1, "ì—°êµ¬ì": name, "ì¤‘ì‹¬ì„±": f"{score:.3f}"}
                        for i, (name, score) in enumerate(top_degree)
                    ])
                    st.dataframe(degree_df, use_container_width=True, hide_index=True)
                
                with col2:
                    st.markdown("#### ğŸŒ‰ ë§¤ê°œ ì¤‘ì‹¬ì„± Top 5")
                    st.caption("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì ì—­í• ")
                    
                    between_df = pd.DataFrame([
                        {"ìˆœìœ„": i+1, "ì—°êµ¬ì": name, "ì¤‘ì‹¬ì„±": f"{score:.3f}"}
                        for i, (name, score) in enumerate(top_betweenness)
                    ])
                    st.dataframe(between_df, use_container_width=True, hide_index=True)
                
                st.markdown("---")
                st.markdown("""
                **ğŸ’¡ í•´ì„ ê°€ì´ë“œ:**
                - **ì—°ê²° ì¤‘ì‹¬ì„±**: ê°’ì´ ë†’ì„ìˆ˜ë¡ ë§ì€ ì—°êµ¬ìì™€ í˜‘ì—… (í—ˆë¸Œ ì—­í• )
                - **ë§¤ê°œ ì¤‘ì‹¬ì„±**: ê°’ì´ ë†’ì„ìˆ˜ë¡ ì„œë¡œ ë‹¤ë¥¸ ì—°êµ¬ ê·¸ë£¹ì„ ì—°ê²° (ë¸Œë¦¿ì§€ ì—­í• )
                """)
            else:
                st.info("ë¶„ì„í•  ê³µì € ê´€ê³„ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ========== íƒ­ 3: ì£¼ìš” ì €ë„ ==========
        with tab3:
            st.markdown("### ğŸ“š ì£¼ìš” ì €ë„")
            
            # ì €ë„ë³„ ë…¼ë¬¸ ìˆ˜ ì§‘ê³„
            journal_counts = df_sorted['journal'].value_counts().head(15)
            journal_counts = journal_counts[journal_counts.index != '']  # ë¹ˆ ì €ë„ ì œì™¸
            
            if len(journal_counts) > 0:
                journal_df = pd.DataFrame({
                    "ìˆœìœ„": range(1, len(journal_counts) + 1),
                    "ì €ë„": journal_counts.index,
                    "ë…¼ë¬¸ ìˆ˜": [f"{x}í¸" for x in journal_counts.values]
                })
                
                st.dataframe(journal_df, use_container_width=True, hide_index=True)
                
                st.markdown("---")
                st.markdown("#### ğŸ“Š ì €ë„ë³„ ë…¼ë¬¸ ë¶„í¬")
                st.bar_chart(journal_counts.head(10))
            else:
                st.info("ì €ë„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    else:
        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ ë³´ì„¸ìš”.")

elif search_button and not search_query.strip():
    st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”!")

else:
    # ì‹œì‘ í™”ë©´
    st.markdown("""
    <h1 style="font-size: 3.5rem; font-weight: bold; color: #1e3a5f; text-align: center; margin-bottom: 0; padding-top: 1rem;">
        ğŸ”‘ Key-Paper Helper
    </h1>
    <h2 style="font-size: 2rem; color: #555; text-align: center; margin-top: 0.5rem; margin-bottom: 1.5rem; font-weight: normal;">
        í•„ë…ë…¼ë¬¸ ì°¾ê¸°
    </h2>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    <div style="font-size: 1.1rem; color: #666; text-align: center; margin-bottom: 2rem;">
        í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ë©´ í•„ë…ë…¼ë¬¸ Top 10ì„ ì¶”ì²œí•´ ë“œë ¤ìš”.<br>
        <span style="color: #999; font-size: 0.9rem;">â€» ì˜ì–´ ë…¼ë¬¸ë§Œ ê²€ìƒ‰ë©ë‹ˆë‹¤ (OpenAlex ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜)</span>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    ### ğŸš€ ì‚¬ìš© ë°©ë²•
    
    1. ğŸ” **ì™¼ìª½**ì—ì„œ ê²€ìƒ‰ì–´ ì…ë ¥
    2. ğŸ“… **ê¸°ê°„** ì„ íƒ
    3. ğŸ” **ê²€ìƒ‰í•˜ê¸°** ë²„íŠ¼ í´ë¦­!
    
    ---
    
    ### ğŸ’¡ ê²€ìƒ‰ íŒ
    
    | ê²€ìƒ‰ ë°©ë²• | ì˜ˆì‹œ | ì„¤ëª… |
    |----------|------|------|
    | ì •í™•í•œ êµ¬ë¬¸ | `"team science"` | ë”°ì˜´í‘œë¡œ ê°ì‹¸ë©´ ì •í™•íˆ ì¼ì¹˜ |
    | OR (ë˜ëŠ”) | `"AI" OR "machine learning"` | ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ |
    | AND (ê·¸ë¦¬ê³ ) | `"deep learning" AND "healthcare"` | ë‘˜ ë‹¤ í¬í•¨ |
    
    ---
    
    ### â„¹ï¸ ì´ ë„êµ¬ëŠ”?
    
    **ì‘ë™ ì›ë¦¬:**
    1. ì…ë ¥í•œ í‚¤ì›Œë“œë¡œ **OpenAlex** (ì„¸ê³„ ìµœëŒ€ í•™ìˆ  DB) ê²€ìƒ‰
    2. ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì„ **ì¸ìš© ìˆ˜** ìˆœìœ¼ë¡œ ì •ë ¬
    3. ê°€ì¥ ë§ì´ ì¸ìš©ëœ **Top 10** ë…¼ë¬¸ ì¶”ì²œ!
    
    > ğŸ’¬ "ì´ ë¶„ì•¼ì—ì„œ ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ë…¼ë¬¸ì´ ë­ì§€?" í•  ë•Œ ì“°ì„¸ìš”!
    """)

# í‘¸í„°
st.divider()
st.markdown("""
<div style="text-align: center; color: #666; padding: 1rem 0;">
    <div style="font-size: 1.2rem; font-weight: bold; margin-bottom: 0.5rem;">
        ğŸ”‘ Key-Paper Helper v3.5
    </div>
    <div style="font-size: 0.9rem; color: #888; margin-bottom: 0.8rem;">
        Powered by OpenAlex API | by ëŒ€í•™ì›ìƒ MJ ğŸ“
    </div>
    <div style="font-size: 1rem; font-style: italic; color: #999; margin-bottom: 0.3rem;">
        "ì´ í‚¤ì›Œë“œë¡œ ë­˜ ë¨¼ì € ì½ì–´ì•¼ í•˜ì§€...?"
    </div>
    <div style="font-size: 0.85rem; color: #aaa;">
        êµ¬ê¸€ ìŠ¤ì¹¼ë¼ 10í˜ì´ì§€ì§¸ í—¤ë§¤ë‹¤ê°€ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤ ğŸ« 
    </div>
</div>
""", unsafe_allow_html=True)
